# 锁

[TOC]



## 内核mutex lock

``` c
struct mutex {
    atomic_long_t       owner;
    spinlock_t      wait_lock;
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER //默认开
    struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
    struct list_head    wait_list;
#ifdef CONFIG_DEBUG_MUTEXES // 默认关
    void            *magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC  // 默认关
    struct lockdep_map  dep_map;
#endif
};
```

所以主要看owner/wait_lock/osq

``` c
void __sched mutex_lock(struct mutex *lock)
{
    might_sleep();

    if (!__mutex_trylock_fast(lock)) // fast path
        __mutex_lock_slowpath(lock);  // slow path
}
// fast path， 直接去修改lock的owner，若成功直接返回。
static __always_inline bool __mutex_trylock_fast(struct mutex *lock)
{
    unsigned long curr = (unsigned long)current;
    unsigned long zero = 0UL;

    if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr)) // cas
        return true;

    return false;
}

// slow path，第一次抢不到锁，就需要spinlock在自己的wait_lock上，然后去check能否抢到锁。
// 或者通过osq lock去抢锁，具体方式未研究。
static noinline void __sched
__mutex_lock_slowpath(struct mutex *lock)
{
    __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);
}
__mutex_lock -> __mutex_lock_common

```

## 用户态mutex lock

pthread_mutex_lock(pthread_mutex_t *mutex)





## spinlock

API有3种：

- 如果只是在普通线程之间同时访问共享对象，使用spin_lock()/spin_unlock()；
- 如果是在中断和普通线程之间同时访问共享对象，并且确信**退出临界区**后要**打开中断**，使用spin_lock_irq()/spin_unlock_irq()；
- 如果是在中断和普通线程之间同时访问共享对象，并且**退出临界区**后要**保持中断**的状态，使用spin_lock_irqsave()/spin_unlock_irqrestore()；

spinlock和中断/抢占的关系，详见《arm64  内核调度/抢占/进程创建   原理源码详解》-持锁时的调度处理。

## qspinlock

合入patch：4.18已经合入了。

​	git show c11090474d70590170cf5fa6afe85864ab

## osq_lock

**思想：**

​	 创建osq lock的时候，只需要创建一个全局queue，其中只有一个tail变量。有人来抢锁的时候，swap操作将自己的核号写上去，同时获取前一个排队者的核号。将前一个人的next指针指向自己，再将自己的prev指针指向前者，就可以欢快的自旋在自己的per_cpu变量lock上，等待前一个人做完后将自己唤醒。

相关结构体：

​	只有一个全局的optimistic_spin_queue，以及per cpu的optimistic_spin_node。

### 基本结构体： optimistic_spin_queue

``` c
struct optimistic_spin_queue {
    /*
     * Stores an encoded value of the CPU # of the tail node in the queue.
     * If the queue is empty, then it's set to OSQ_UNLOCKED_VAL.
     */
    atomic_t tail;
};
```

### 基本结构体： optimistic_spin_node

```c
struct optimistic_spin_node {
    struct optimistic_spin_node *next, *prev;
    int locked; /* 1 if lock acquired */
    int cpu; /* encoded CPU # + 1 value */         // 用来记录自己的核号
};
```

<img src="D:\at_work\Documents\我的总结文档\images\osq_lock流程梳理.png" alt="osq_lock流程梳理" style="zoom:150%;" />

## RCU同步机制

Read -> Copy -> Update。

### RCU特点

读：

- 不需要获取锁，不需要atomic，不需要barrier

写：

- 多个writer之间需要同步，开销较大
- 一个writer需要等所有reader都完成后，才可以进行update更新。



### RCU源码

#### RCU声明

``` c
struct fdtable {
    unsigned int max_fds;
    struct file __rcu **fd;     // __rcu只是用于编译器检查的，告诉编译器这是个rcu保护的变量。
    unsigned long *close_on_exec;
    unsigned long *open_fds;
    unsigned long *full_fds_bits;
    struct rcu_head rcu;     // 真正用到rcu的结构体，会包含rcu_head成员。
};
```

struct fdtable, struct file都是受RCU保护的结构体。

#### RCU读

先声明一个临界区，然后通过`rcu_dereference`函数来获取rcu成员变量指针的一份copy，因为不能直接操作rcu成员。

``` c
rcu_read_lock();  // 获取读锁，进入读临界区（具体实现没有锁）
p1 = rcu_dereference(p);  // 访问受rcu保护的指针时，必须使用rcu_deference()接口
				// 作用:把指针p赋值给p1
if (p1 != NULL) {
    printk("%d\n", p1->field);
}
rcu_read_unlock();   // 释放读锁，退出读临界区（具体实现没有锁）
```

全局只有一个rcu，会用来保护所有rcu机制内的成员指针。

其中的`rcu_read_lock()`和`rcu_read_unlock()`用来标识临界区，表示在这期间是rcu读锁的操作，不允许抢占。在不开`CONFIG_DEBUG_LOCK_ALLOC`时，作用仅仅是开关抢占，若系统中不使能抢占，则几乎等于空函数。 

``` c
static inline void rcu_read_unlock(void)
{
    RCU_LOCKDEP_WARN(!rcu_is_watching(),
             "rcu_read_unlock() used illegally while idle");
    __release(RCU); // 给编译器的hint而已，__acquire必须与__release配对
    __rcu_read_unlock();  // 仅仅是开抢占而已。
    rcu_lock_release(&rcu_lock_map); // 若不开CONFIG_DEBUG_LOCK_ALLOC，则为空函数。
    	-> lock_release -> __lock_release
}
rcu_read_lock也差不多。
```

`rcu_dereference`这个函数才和具体的rcu变量有关，去获取rcu指针。



#### RCU写

通过`rcu_assign_pointer`来进行rcu成员变量的更新，这里会等所有读锁释放后再进行更新。

``` c
#define rcu_assign_pointer(p, v)                                              \
do {                                                                          \
        uintptr_t _r_a_p__v = (uintptr_t)(v);                                 \
        rcu_check_sparse(p, __rcu);                                           \
                                                                              \
        if (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)        \
                WRITE_ONCE((p), (typeof(p))(_r_a_p__v));                      \
        else                                                                  \
                smp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \
} while (0)
```



``` c
static int do_dup2(struct files_struct *files,
    struct file *file, unsigned fd, unsigned flags)
__releases(&files->file_lock)
{
    struct file *tofree;
    struct fdtable *fdt;

    fdt = files_fdtable(files);
    tofree = fdt->fd[fd];
    if (!tofree && fd_is_open(fd, fdt))
        goto Ebusy;
    get_file(file);
    rcu_assign_pointer(fdt->fd[fd], file);
    __set_open_fd(fd, fdt);
    if (flags & O_CLOEXEC)
        __set_close_on_exec(fd, fdt);
    else
        __clear_close_on_exec(fd, fdt);
    spin_unlock(&files->file_lock);

    if (tofree)
        filp_close(tofree, files);

    return fd;
```

### RCU调用的底层锁

# atomic

## 代码封装层次

从需求方开始，直接调用的接口为：

``` c
1. atomic_long_add_unless  // 在"include/asm-generic/atomic-long.h"中解析long对应的size大小
2. ->  atomic64_add_unless  // 在"include/asm-generic/atomic-instrumented.h"中进行一次封装，若开启了kasan和kcsan特性(动态地址和动态数据），则做atomic之前首先进行检查，不影响atomic结构体本身的访问pattern，使用了额外的内存空间。
3. -> arch_atomic64_add_unless // "include/linux/atomic/atomic-arch-fallback.h"中的函数实现
	-> arch_atomic64_fetch_add_unless // 本地函数调用
    -> arch_atomic64_read + arch_atomic64_try_cmpxchg // 然后就真正区分架构了 
						       
4. arch_atomic64_read // 不会调用arm64的原子指令，
    			// 则"arch/arm64/include/asm/atomic.h"中直接定义了
   -> arch_atomic_read
        -> __READ_ONCE((v)->counter)
4. arch_atomic64_try_cmpxchg // 会调用arm64的原子指令
    -> arch_atomic64_try_cmpxchg //
    	-> arch_atomic64_cmpxchg
5.    		-> arch_cmpxchg    // 具体指针文件： arch/arm64/include/asm/cmpxchg.h
    			-> __cmpxchg_wrapper //
    				-> __cmpxchg##sfx
    					-> __cmpxchg_case##sfx##_64
    						-> __lse_ll_sc_body(**)
6. __lse_ll_sc_body  // 在"arch/arm64/include/asm/lse.h"，会判断当前系统是否使用lse，
    				 // 若使用则调用__lse_##op,否则调用__ll_sc_##op
7.    ->最底层的实现  // "arch/arm64/include/asm/atomic_lse.h" 或者atomic_ll_sc.h。
```

​	其中，

`try_cmpxchg`的实现在第4层， arch/arm64/include/asm/atomic.h中；

`cmpxchg`的实现在第7层，arch/arm64/include/asm/atomic_lse.h中。

## lse指令

arch/arm64/include/asm/atomic_lse.h中可以看到，如果忽略atomic指令只看atomic64指令的话（应该只是32 bit data长度的区别），共定义了以下几类指令。

| 序号 | 操作类别               | 展开                                                  | 汇编                               |
| ---- | ---------------------- | ----------------------------------------------------- | ---------------------------------- |
| 1    | ATOMIC64_OP            | 比如： atomic64_add(i, v)                             | stadd  x0, v->counter // x0存放是i |
| 2*   | ATOMIC64_FETCH_OP      | 比上面多了个mb指令 atomic64_fetch_ldadd_acquire(i, v) | ldadd a x0,x0, v->counter          |
| 3    | ATOMIC64_OP_ADD_RETURN |                                                       |                                    |
| 4    | ATOMIC64_FETCH_OP_AND  |                                                       |                                    |
| 5    | ATOMIC64_OP_SUB_RETURN |                                                       |                                    |
| 6    | ATOMIC64_FETCH_OP_SUB  |                                                       |                                    |
| 7    | __CMPXCHG_CASE         |                                                       |                                    |
| 8    | __CMPXCHG_DBL          |                                                       |                                    |

fetch类的指令，包含了4种barrier原语：

	1. relaxed        2. acquire     3. release      4. NULL

其中，relaxed表示不需要barrier，acquire/release都是单边barrier，用a/l来表示，NULL表示双边barrier都需要。

``` c
#define ATOMIC64_FETCH_OPS(op, asm_op)                  \
    ATOMIC64_FETCH_OP(_relaxed,   , op, asm_op)         \
    ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")       \
    ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")       \
    ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
```

# futex

## why use futex？

​	fast userspace mutex，是用户态的锁，用于进程间的同步。细粒度可以使用spinlock，但粒度大到需要进程睡眠等待，用spinlock盲等的效率就很低了。而进程睡眠及唤醒这种事情，只能是内核才能干，传统的进程同步使用semaphore，socket，msgqueue等，调用时都需要trap到内核去。在低竞争场景下效率很低，往返一次内核的开销是无用功。那么，需要这种机制：

1. 内核要支持锁粒度的唤醒/睡眠操作，进程挂起时要能管理等待队列；
2. 用户态在无冲突时，可以直接原子操作抢锁&释放锁。

futex应运而生，实现机制是mmap一段内存，让内核和用户态共享，其中包含futex变量。当进程尝试进入/退出临界区时，看一下futex变量，若没有竞争则直接修改futex就不用系统调用了；若有竞争，则仍需要syscall去wakeup等。

## what is futex？

​		futex对应的也是系统调用接口。

用户态使用方式：

``` c
// 基本方案
// 在uaddr指向的这个锁变量上挂起等待（仅当*uaddr==val时）
int futex_wait(int *uaddr, int val);
// 唤醒n个在uaddr指向的锁变量上挂起等待的进程
int futex_wake(int *uaddr, int n);
```

``` c
// 条件futex
In Thread1:

pthread_mutex_lock(&m_mutex); 
pthread_cond_wait(&m_cond,&m_mutex); // 等待条件
pthread_mutex_unlock(&m_mutex);

In Thread2:

pthread_mutex_lock(&m_mutex);
pthread_cond_signal(&m_cond); // 使能条件
pthread_mutex_unlock(&m_mutex);
```

``` c
// 带优先级的futex
int futex_lock_pi(int *uaddr);
int futex_trylock_pi(int *uaddr);
int futex_unlock_pi(int *uaddr);
// 当一个高优先级任务在等待低优先级任务释放锁的时候，若低pi任务一直被中pi任务们抢占则迟迟完成不了，导致高pi任务一直在等。
// 为了解决这个问题，引入lock_pi.
```

``` c
// requeue的futex
// 唤醒 n个等待进程，再把其中n2个进程移到另一个等待队列中去。
int futex_requeue(int *uaddr, int n, int *uaddr2, int n2);
int futex_cmp_requeue(int *uaddr, int n, int *uaddr2, int n2, int val); // 条件成立再操作
```









# 688 硬件实现

1. 之前存在的问题：

   - 锁分为条件的和非条件的两大类:
   - unconditional在多核场景下性能没问题，而conditional性能非常差。
   - 原因： 1620的锁操作都是在L3做的。conditional需要先比较，成功后才会改写，而之前：
     - 在比较的时候就已经去获取E态，就需要将别人获取的状态打掉，导致竞争激烈，成功率低。
     - L3T与L3D的交互需要时间，这里compare需要交互一次，swap又需要交互一次，在688上就会性能更差。
     - 同cacheline的其他操作，在conditional之后，依然没有E态，所以再操作时又要到L3去要数据。
   - 因此，688的方案考虑是，将conditional的atomic操作前移到核内去做，从而：
     - 若核内有S/E态，可以直接比较，比较不通过则无需出核了。
     - 减少L3T与L3D的交互，compare和swap都在核内做掉。
     - atomic做完后，整条cacheline都已经在核内了，后续操作也可以不用出核。

   - 另外的优化点有：
     - 增加E态持续的时间，防止好不容易获取的E态又很快被抢走。 -- snoop delay
       - MATA做snoop delay，增加home L3T的持锁时间；    -- 减弱不同cluster/跨die/跨P的竞争强度
       - L3T做snoop delay，增加cluster内core的持锁时间。  -- 提升同cluster内抢锁成功率
     - 增加同cluster内E态持续时间。 -- L3 remote给home返回compack前增加delay
       - E态迁移。 -- 同cluster内的L3若有E态，则可以不修改L3，直接将c0的E态迁移给c1。同时，若该cluster内E态迁移达到阈值后，就不再迁移了，给其他cluster机会。
       - remote从home获取E态后，延迟一段时间再返回compAck，这段时间内，remote L3一直有E态，从而为同cluster内不同core的E态迁移提供时间窗口。
       - ps: snoop delay与compack delay的区别： 前者增加core内持锁时间，后者增加cluster内持锁时间。
     - 优化单核场景 -- 单核stash（MATA/L3）
       - 优化场景：单核在atomic，但是有其他核会read该cacheline。正常情况是双S态，那么单核atomic时就需要重新获取E态。

### load + cas的优化

load+cas的瓶颈点：

1. load拿S态，到了cas还要重新拿E，而直接ldr拿E就不会两次上Soc；
2. 抢锁激烈场景下，load后可能被别人改掉，导致cas的成功率不高。若是拿E态，可以有snoop delay机制防止别人短时间内修改。

1650的优化：

1. 1650 table移到核内做，不像之前在L3做 要管多个核，地址过多场景得到改善了

## 688的atomic性能优化点汇总

详见D:\dbox-doc\PhosphorV688\Docs\01.KIA\1.3.Verification\1.3.2.EDA\1.3.2.2.Blocks\大UT\Testbench\性能测试\CAS优化测试\Atomic优化分析汇总.eddx

### CPU优化点

![image-20210818094050489](D:\at_work\Documents\我的总结文档\images\image-20210818094050489.png)

### L3T优化点

![image-20210818101821880](C:\Users\h00424781\AppData\Roaming\Typora\typora-user-images\image-20210818101821880.png)

注意： 

1. atomic地址是片内share的。
2. 

### MATA优化点

![image-20210818101833078](C:\Users\h00424781\AppData\Roaming\Typora\typora-user-images\image-20210818101833078.png)

## 纯CAS

cas的用法是：　cas(*location, old_value, new_value)

​	其中后两者都是数值，已经在之前取过了，因此，这里需要进行load/store操作的地址只有一个，就是第一个指针。

![image-20210818092626988](D:\at_work\Documents\我的总结文档\images\image-20210818092626988.png)

数据流分析：

    1. 核内的atomic主战场是L2（bypass L1），计算则是在LSU做的。

 	2. LSU发现（或者是decode？）指令为纯cas，则首先下发一个RS prefetch到L2，不管cas地址状态是啥。L2会自行判断，若I态则下发给L3一个RS prefetch；若S/E态则直接drop掉这个prefetch请求。
	3. L3收到RS prf后，查找atomic table，若决定给L2升级，则回复一个带snoop delay hint的compdata给L2.
	4. L2收到hint，再回复带hint的compack给L3.
	5. L3收到L2的hint后，就开始snoop delay的计时了，计时结束前都不会给这个L2发送同地址的snoop操作。
	6. 再回来看L2，当L2收到compdata_E后，I态修改为E态，同时fill data到LSU（prf RS操作的响应）
	7. LSU的CAS操作会等待RS完成后才能下发（因为同地址load阻塞store），因此收到fill data后才会继续。此时下发CAS给L2.
	8. L2先进行compare操作，若fail则操作结束。则success，因此已经有E态了，所以直接swap改写，完成后fill data给LSU，LSU最后返回atomic done给L2.
	9. over

## snoop delay时间计算

![image-20210818095211787](D:\at_work\Documents\我的总结文档\images\image-20210818095211787.png)

# 1650 硬件实现

## 与前一代的差别

1. atomic table搞成了2层，分别放到了核内 + L3；
2. snp delay转到了核内来做，由L2保证，当L3发了snp后L2会延迟回ack；
3. 